## [1.3  Formulating Abstractions with Higher-Order Procedures](contents.html#sec_1.3)

We have seen that procedures are, in effect, abstractions that describe
compound operations on numbers independent of the particular numbers. For
example, when we

{{ 1.3.1.clj }}

we are not talking about the cube of a particular number, but rather about a
method for obtaining the cube of any number. Of course we could get along
without ever defining this procedure, by always writing expressions such as

{{ 1.3.2.clj }}

and never mentioning `cube` explicitly. This would place us at a serious
disadvantage, forcing us to work always at the level of the particular
operations that happen to be primitives in the language (multiplication, in
this case) rather than in terms of higher-level operations. Our programs would
be able to compute cubes, but our language would lack the ability to express
the concept of cubing. One of the things we should demand from a powerful
programming language is the ability to build abstractions by assigning names
to common patterns and then to work in terms of the abstractions directly.
Procedures provide this ability. This is why all but the most primitive
programming languages include mechanisms for defining procedures.

Yet even in numerical processing we will be severely limited in our ability to
create abstractions if we are restricted to procedures whose parameters must
be numbers. Often the same programming pattern will be used with a number of
different procedures. To express such patterns as concepts, we will need to
construct procedures that can accept procedures as arguments or return
procedures as values. Procedures that manipulate procedures are called
_higher-order procedures_. This section shows how higher-order procedures can
serve as powerful abstraction mechanisms, vastly increasing the expressive
power of our language.

### [1.3.1  Procedures as Arguments](contents.html#sec_1.3.1)

Consider the following three procedures. The first computes the sum of the
integers from `a` through `b`:

{{ 1.3.1.1.clj }}

The second computes the sum of the cubes of the integers in the given range:

{{ 1.3.1.2.clj }}

The third computes the sum of a sequence of terms in the series

\\[ \frac{1}{1 \cdot 3} + \frac{1}{5 \cdot 7} + \frac{1}{9 \cdot 11} + \dotsb \\]

which converges to \\( \pi/8 \\) (very slowly):49

{{ 1.3.1.3.clj }}

These three procedures clearly share a common underlying pattern. They are for
the most part identical, differing only in the name of the procedure, the
function of `a` used to compute the term to be added, and the function that
provides the next value of `a`. We could generate each of the procedures by
filling in slots in the same template:

{{ 1.3.1.4.clj }}

The presence of such a common pattern is strong evidence that there is a
useful abstraction waiting to be brought to the surface. Indeed,
mathematicians long ago identified the abstraction of _summation of a series_
and invented "sigma notation," for example

\\[ \sum_{n=a}^{b} f(n) = f(a) + \dotsb + f(b) \\]

to express this concept. The power of sigma notation is that it allows
mathematicians to deal with the concept of summation itself rather than only
with particular sums -- for example, to formulate general results about sums
that are independent of the particular series being summed.

Similarly, as program designers, we would like our language to be powerful
enough so that we can write a procedure that expresses the concept of
summation itself rather than only procedures that compute particular sums. We
can do so readily in our procedural language by taking the common template
shown above and transforming the "slots" into formal parameters:

{{ 1.3.1.5.clj }}

Notice that `sum` takes as its arguments the lower and upper bounds `a` and
`b` together with the procedures `term` and `next`. We can use `sum` just as
we would any procedure. For example, we can use it (along with a procedure
`inc` that increments its argument by 1) to define `sum-cubes`:

{{ 1.3.1.6.clj }}

Using this, we can compute the sum of the cubes of the integers from 1 to 10:

{{ 1.3.1.7.clj }}

With the aid of an identity procedure to compute the term, we can define 
`sum-integers` in terms of `sum`:

{{ 1.3.1.8.clj }}

Then we can add up the integers from 1 to 10:

{{ 1.3.1.9.clj }}

We can also define `pi-sum` in the same way:50

{{ 1.3.1.10.clj }}

Using these procedures, we can compute an approximation to \\( \pi \\):

{{ 1.3.1.11.clj }}

Once we have `sum`, we can use it as a building block in formulating further
concepts. For instance, the definite integral of a function _f_  between the
limits _a_ and _b_ can be approximated numerically using the formula

\\[ \int_{a}^{b} f = \left [ f\left (a+\frac{dx}{2} \right ) + 
f\left( a + dx + \frac{dx}{2} \right ) + 
f\left( a + 2dx + \frac{dx}{2} \right ) +
\dotsb \right ] dx \\]

for small values of _d__x_. We can express this directly as a procedure:

{{ 1.3.1.12.clj }}

(The exact value of the integral of `cube` between 0 and 1 is 1/4.)

**Exercise 1.29.**  Simpson's Rule is a more accurate method of numerical integration
than the method illustrated above. Using Simpson's Rule, the integral of a function _f_
between _a_ and _b_ is approximated as 

\\[ \frac{h}{3}[y\_{0}+4y\_{1}+2y\_{2}+4y\_{3}+2y\_{4}+\dotsb+2y\_{n-2}+4y\_{n-1}+y\_{n}] \\]

where \\( h=(b-a)/n \\), for some even integer _n_, and \\( y\_{k} = f(a+kh) \\).
(Increasing _n_ increases the accuracy of the approximation.)
Define a procedure that takes as arguments _f_, _a_, _b_, and _n_ and returns
the value of the integral, computed using Simpson's Rule. Use your procedure
to integrate `cube` between 0 and 1 (with _n_ = 100 and _n_ = 1000), and
compare the results to those of the `integral` procedure shown above.

**Exercise 1.30.**  The `sum` procedure above generates a linear recursion. The 
procedure can be rewritten so that the sum is performed iteratively. Show how to
do this by filling in the missing expressions in the following definition:

{{ 1.3.1.13.clj }}

**Exercise 1.31.**     
a.  The `sum` procedure is only the simplest of a vast number of similar
abstractions that can be captured as higher-order procedures.51 Write an
analogous procedure called `product` that returns the product of the values of
a function at points over a given range. Show how to define `factorial` in
terms of `product`. Also use `product` to compute approximations to
\\( \pi \\) using the formula52

\\[ \frac{\pi}{4}=\frac{2\cdot4\cdot4\cdot6\cdot6\cdot8\dotsb}
                  {3\cdot3\cdot5\cdot5\cdot7\cdot7\dotsb} \\]

b.  If your `product` procedure generates a recursive process, write one that
generates an iterative process. If it generates an iterative process, write
one that generates a recursive process.

**Exercise 1.32.**  a. Show that `sum` and `product` (exercise 1.31) are both special
cases of a still more general notion called `accumulate` that combines a collection
of terms, using some general accumulation function:

{{ 1.3.1.14.clj }}

`Accumulate` takes as arguments the same term and range specifications as
`sum` and `product`, together with a `combiner` procedure (of two arguments)
that specifies how the current term is to be combined with the accumulation of
the preceding terms and a `null-value` that specifies what base value to use
when the terms run out. Write `accumulate` and show how `sum` and `product`
can both be defined as simple calls to `accumulate`.

b. If your `accumulate` procedure generates a recursive process, write one
that generates an iterative process. If it generates an iterative process,
write one that generates a recursive process.

**Exercise 1.33.**  You can obtain an even more general version of `accumulate`
(exercise 1.32) by introducing the notion of a _filter_ on the terms to be combined. 
That is, combine only those terms derived from values in the range that satisfy a
specified condition. The resulting `filtered-accumulate` abstraction takes the same
arguments as accumulate, together with an additional predicate of one argument that 
specifies the filter. Write `filtered-accumulate` as a procedure. Show how to express
the following using `filtered-accumulate`:

a. the sum of the squares of the prime numbers in the interval _a_ to _b_
(assuming that you have a `prime?` predicate already written)

b. the product of all the positive integers less than _n_ that are relatively
prime to _n_ (i.e., all positive integers _i_ &lt; _n_ such that
_GCD_(_i_, _n_) = 1).

### [1.3.2  Constructing Procedures Using `fn`](contents.html#sec_1.3.2)

In using `sum` as in section 1.3.1, it seems terribly awkward to have to
define trivial procedures such as `pi-term` and `pi-next` just so we can use
them as arguments to our higher-order procedure. Rather than define `pi-next`
and `pi-term`, it would be more convenient to have a way to directly specify
"the procedure that returns its input incremented by 4" and "the procedure
that returns the reciprocal of its input times its input plus 2." We can do
this by introducing the special form `fn`, which creates procedures. Using
`fn` we can describe what we want as

{{ 1.3.2.1.clj }}

and

{{ 1.3.2.2.clj }}

Then our `pi-sum` procedure can be expressed without defining any auxiliary
procedures as

{{ 1.3.2.3.clj }}

Again using `fn`, we can write the `integral` procedure without having to
define the auxiliary procedure `add-dx`:

{{ 1.3.2.4.clj }}

In general, `fn` is used to create procedures in the same way as `defn`,
except that no name is specified for the procedure:

{{ 1.3.2.5.clj }}

The resulting procedure is just as much a procedure as one that is created
using `defn`. The only difference is that it has not been associated with
any name in the environment. In fact,

{{ 1.3.2.6.clj }}

is equivalent to

{{ 1.3.2.7.clj }}

We can read a `fn` expression as follows:

{{ 1.3.2.8.clj }}

Like any expression that has a procedure as its value, a `fn` expression
can be used as the operator in a combination such as

{{ 1.3.2.9.clj }}

or, more generally, in any context where we would normally use a procedure
name.53

#### [Using `let` to create local variables](book-Z-H-4.html#sec_Temp_100)

Another use of `fn` is in creating local variables. We often need local
variables in our procedures other than those that have been bound as formal
parameters. For example, suppose we wish to compute the function

\\[ f(x,y) = x(1+xy)^{2} +y(1-y)+(1+xy)(1-y) \\]

which we could also express as

\\[ \begin{array}{l} a=1+xy \\ b = 1-y \\ f(x,y) = xa^{2} +yb +ab \end{array} \\]

In writing a procedure to compute _f_, we would like to include as local
variables not only _x_ and _y_ but also the names of intermediate quantities
like _a_ and _b_. One way to accomplish this is to use an auxiliary procedure
to bind the local variables:

{{ 1.3.2.10.clj }}

Of course, we could use a `fn` expression to specify an anonymous
procedure for binding our local variables. The body of `f` then becomes a
single call to that procedure:

{{ 1.3.2.11.clj }}

This construct is so useful that there is a special form called `let` to make
its use more convenient. Using `let`, the `f` procedure could be written as

{{ 1.3.2.12.clj }}

The general form of a `let` expression is

{{ 1.3.2.13.clj }}

which can be thought of as saying

> let
>
> <var 1> have the value <exp 1> and
>
> <var 2> have the value <exp 2> and
>
> [...]
>
> <var n> have the value <exp n>
> 
> in
>
> <body>

The first part of the `let` expression is a vector of name-expression pairs.
When the `let` is evaluated, each name is associated with the value of the
corresponding expression. The body of the `let` is evaluated with these names
bound as local variables. The way this happens is that the `let` expression is
interpreted as an alternate syntax for

{{ 1.3.2.14.clj }}

No new mechanism is required in the interpreter in order to provide local
variables. A `let` expression is simply syntactic sugar for the underlying
`lambda` application.

We can see from this equivalence that the scope of a variable specified by a
`let` expression is the body of the `let`. This implies that:

  * `Let` allows one to bind variables as locally as possible to where they are to be
  used. For example, if the value of `x` is 5, the value of the expression

{{ 1.3.2.15.clj }}

is 38. Here, the `x` in the body of the `let` is 3, so the value of the `let`
expression is 33. On the other hand, the `x` that is the second argument to
the outermost `+` is still 5.

  * The variables' values are computed outside the `let`. This matters when the
  expressions that provide the values for the local variables depend upon variables
  having the same names as the local variables themselves. For example, if the value
  of `x` is 2, the expression

{{ 1.3.2.16.clj }}

will have the value 12 because, inside the body of the `let`, `x` will be 3
and `y` will be 4 (which is the outer `x` plus 2).

Sometimes we can use internal definitions to get the same effect as with
`let`. For example, we could have defined the procedure `f` above as

{{ 1.3.2.17.clj }}

We prefer, however, to use `let` in situations like this and to use internal
`defn` only for internal procedures.54

**Exercise 1.34.**  Suppose we define the procedure

{{ 1.3.2.18.clj }}

Then we have

{{ 1.3.2.19.clj }}

What happens if we (perversely) ask the interpreter to evaluate the
combination `(f f)`? Explain.

### [1.3.3  Procedures as General Methods](contents.html#sec_1.3.3)

We introduced compound procedures in section
[1.1.4](book-Z-H-10.html#%_sec_1.1.4) as a mechanism for abstracting patterns
of numerical operations so as to make them independent of the particular
numbers involved. With higher-order procedures, such as the `integral`
procedure of section 1.3.1, we began to see a more powerful kind of
abstraction: procedures used to express general methods of computation,
independent of the particular functions involved. In this section we discuss
two more elaborate examples -- general methods for finding zeros and fixed
points of functions -- and show how these methods can be expressed directly as
procedures.

#### [Finding roots of equations by the half-interval method](book-Z-H-4.html#sec_Temp_103)

The _half-interval method_ is a simple but powerful technique for finding
roots of an equation \\( f(x)=0 \\), where _f_ is a continuous function. The
idea is that, if we are given points _a_ and _b_ such that \\( f(a) < 0 < f(b) \\),
then _f_ must have at least one zero between _a_ and _b_. To locate
a zero, let _x_ be the average of _a_ and _b_ and compute \\( f(x) \\). If
\\( f(x) > 0 \\), then _f_ must have a zero between _a_ and _x_. If \\( f(x) < 0 \\),
then _f_ must have a zero between _x_ and _b_. Continuing in this way, we can
identify smaller and smaller intervals on which _f_ must have a zero. When we
reach a point where the interval is small enough, the process stops. Since the
interval of uncertainty is reduced by half at each step of the process, the
number of steps required grows as \\( \Theta(\log(L/T)) \\),
where _L_ is the length of the original interval and _T_ is the error
tolerance (that is, the size of the interval we will consider "small
enough"). Here is a procedure that implements this strategy:

{{ 1.3.3.1.clj }}

We assume that we are initially given the function _f_ together with points at
which its values are negative and positive. We first compute the midpoint of
the two given points. Next we check to see if the given interval is small
enough, and if so we simply return the midpoint as our answer. Otherwise, we
compute as a test value the value of _f_ at the midpoint. If the test value is
positive, then we continue the process with a new interval running from the
original negative point to the midpoint. If the test value is negative, we
continue with the interval from the midpoint to the positive point. Finally,
there is the possibility that the test value is 0, in which case the midpoint
is itself the root we are searching for.

To test whether the endpoints are "close enough" we can use a procedure
similar to the one used in section [1.1.7](book-Z-H-10.html#%_sec_1.1.7) for
computing square roots:55

{{ 1.3.3.2.clj }}

`Search` is awkward to use directly, because we can accidentally give it
points at which _f_'s values do not have the required sign, in which case we
get a wrong answer. Instead we will use `search` via the following procedure,
which checks to see which of the endpoints has a negative function value and
which has a positive value, and calls the `search` procedure accordingly. If
the function has the same sign on the two given points, the half-interval
method cannot be used, in which case the procedure signals an error.56

{{ 1.3.3.3.clj }}

The following example uses the half-interval method to approximate
\\( \pi \\) as the root between 2 and 4 of \\( \sin(x) = 0 \\):

{{ 1.3.3.4.clj }}

Here is another example, using the half-interval method to search for a root
of the equation \\( x^{3} - 2x -3 = 0 \\) between 1 and 2:

{{ 1.3.3.5.clj }}

#### [Finding fixed points of functions](book-Z-H-4.html#sec_Temp_106)

A number _x_ is called a _fixed point_ of a function _f_ if _x_ satisfies the
equation \\( f(x)=x \\). For some functions _f_ we can locate a fixed point by
beginning with an initial guess and applying _f_ repeatedly,

\\[ f(x),f(f(x)),f(f(f(x))),\dotsc \\]

until the value does not change very much. Using this idea, we can devise a
procedure `fixed-point` that takes as inputs a function and an initial guess
and produces an approximation to a fixed point of the function. We apply the
function repeatedly until we find two successive values whose difference is
less than some prescribed tolerance:

{{ 1.3.3.6.clj }}

For example, we can use this method to approximate the fixed point of the
cosine function, starting with 1 as an initial approximation:57

{{ 1.3.3.7.clj }}

Similarly, we can find a solution to the equation \\( y = \sin(y) + \cos(y) \\):

{{ 1.3.3.8.clj }}

The fixed-point process is reminiscent of the process we used for finding
square roots in section [1.1.7](book-Z-H-10.html#%_sec_1.1.7). Both are based
on the idea of repeatedly improving a guess until the result satisfies some
criterion. In fact, we can readily formulate the square-root computation as a
fixed-point search. Computing the square root of some number _x_ requires
finding a _y_ such that \\( y^{2}=x \\). Putting this equation into the equivalent
form \\( y=x/y \\), we recognize that we are looking for a fixed point of the
function58 \\( y \mapsto x/y \\), and we can therefore try to compute square roots as

{{ 1.3.3.9.clj }}

Unfortunately, this fixed-point search does not converge. Consider an initial
guess \\(y\_{1}\\). The next guess is \\(y\_{2}=x/y\_{1} \\) and the next guess 
is \\(y\_{3}=x/y\_{2}=x/(x/y\_{1})=y\_{1}\\). This results in an infinite loop
in which the two guesses \\(y\_{1}\\) and \\(y\_{2}\\) repeat over and over,
oscillating about the answer.

One way to control such oscillations is to prevent the guesses from changing
so much. Since the answer is always between our guess _y_ and _x_/_y_, we can
make a new guess that is not as far from _y_ as _x_/_y_ by averaging _y_ with
_x_/_y_, so that the next guess after _y_ is (1/2)(_y_ \+ _x_/_y_) instead of
_x_/_y_. The process of making such a sequence of guesses is simply the
process of looking for a fixed point of \\( y \mapsto (1/2)(y+x/y) \\):

{{ 1.3.3.10.clj }}

(Note that \\(y=(1/2)(y + x/y)\\) is a simple transformation of the
equation \\(y=x/y\\); to derive it, add _y_ to both sides of the equation
and divide by 2.)

With this modification, the square-root procedure works. In fact, if we
unravel the definitions, we can see that the sequence of approximations to the
square root generated here is precisely the same as the one generated by our
original square-root procedure of section
[1.1.7](book-Z-H-10.html#%_sec_1.1.7). This approach of averaging successive
approximations to a solution, a technique we that we call _average damping_,
often aids the convergence of fixed-point searches.

**Exercise 1.35.**  Show that the golden ratio \\( \phi \\)
(section [1.2.2](book-Z-H-11.html#%_sec_1.2.2)) is a fixed point of the 
transformation \\( x \mapsto 1+1/x \\), and use this fact to compute \\( \phi \\)
by means of the `fixed-point` procedure. 

**Exercise 1.36.**  Modify `fixed-point` so that it prints the sequence of approximations
it generates, using the `println` primitive shown in exercise [1.22](book-Z-H-11.html#%_thm_1.22).
Then find a solution to \\( x^{x}=1000 \\) by finding a fixed point of \\( x \mapsto 
\log(1000)/\log(x) \\). (Use the `Math/log` procedure, which computes natural logarithms.)
Compare the number of steps this takes with and without average damping. (Note that you cannot
start `fixed-point` with a guess of 1, as this would cause division by \\(\log(1)=0\\).) 

**Exercise 1.37.**  a. An infinite _continued fraction_ is an expression of the form 

\\[ f = \frac{N\_{1}}{D\_{1} + \frac{N\_{2}}{D\_{2}+\frac{N\_{3}}{D\_{3} + \dotsb}}} \\]

As an example, one can show that the infinite continued fraction expansion
with the \\(N\_{i}\\) and the \\( D\_{1} \\) all equal to 1 produces
\\( 1/\phi \\), where \\( \phi \\) is the golden ratio
(described in section [1.2.2](book-Z-H-11.html#%_sec_1.2.2)). One way to
approximate an infinite continued fraction is to truncate the expansion after
a given number of terms. Such a truncation -- a so-called _k-term finite
continued fraction_ \-- has the form

\\[  \frac{N\_{1}}{D\_{1} + \frac{N\_{2}}{\dotsb+\frac{N\_{k}}{D\_{k}}}} \\]

Suppose that `n` and `d` are procedures of one argument (the term index _i_)
that return the \\(N\_{i}\\) and \\(D\_{i}\\) of the terms of the continued fraction.
Define a procedure `cont-frac` such that evaluating `(cont-frac n d k)`
computes the value of the _k_-term finite continued fraction. Check your
procedure by approximating \\( 1/\phi \\) using

{{ 1.3.3.11.clj }}

for successive values of `k`. How large must you make `k` in order to get an
approximation that is accurate to 4 decimal places?

b. If your `cont-frac` procedure generates a recursive process, write one that
generates an iterative process. If it generates an iterative process, write
one that generates a recursive process.

**Exercise 1.38.**  In 1737, the Swiss mathematician Leonhard Euler published a 
memoir _De Fractionibus Continuis_, which included a continued fraction expansion
 for \\( e-2 \\), where _e_ is the base of the natural logarithms. In this fraction,
the \\(N\_{i}\\) are all 1, and the \\(D\_{i}\\) are successively 1, 2, 1, 1, 4, 1, 1,
6, 1, 1, 8, ... . Write a program that uses your `cont-frac` procedure from exercise 1.37
to approximate _e_, based on Euler's expansion. 

**Exercise 1.39.**  A continued fraction representation of the tangent function was published
in 1770 by the German mathematician J.H. Lambert: 

\\[ \tan(x) = \frac{x}{1 - \frac{x^{2}}{3-\frac{x^{2}}{5-\dotsb}}} \\]

where _x_ is in radians. Define a procedure `(tan-cf x k)` that computes an
approximation to the tangent function based on Lambert's formula. `K`
specifies the number of terms to compute, as in exercise 1.37.

### [1.3.4  Procedures as Returned Values](contents.html#sec_1.3.4)

The above examples demonstrate how the ability to pass procedures as arguments
significantly enhances the expressive power of our programming language. We
can achieve even more expressive power by creating procedures whose returned
values are themselves procedures.

We can illustrate this idea by looking again at the fixed-point example
described at the end of section 1.3.3. We formulated a new version of the
square-root procedure as a fixed-point search, starting with the observation
that \\( \sqrt{x} \\) is a fixed-point of the function \\( y \mapsto x/y \\).
Then we used average damping to make the approximations converge. Average damping
is a useful general technique in itself. Namely, given a function _f_, we consider
the function whose value at _x_ is equal to the average of _x_ and \\( f(x) \\).

We can express the idea of average damping by means of the following
procedure:

{{ 1.3.4.1.clj }}

`Average-damp` is a procedure that takes as its argument a procedure `f` and
returns as its value a procedure (produced by the `fn`) that, when applied
to a number `x`, produces the average of `x` and `(f x)`. For example,
applying `average-damp` to the `square` procedure produces a procedure whose
value at some number _x_ is the average of _x_ and \\(x^{2}\\). Applying this
resulting procedure to 10 returns the average of 10 and 100, or 55:59

{{ 1.3.4.2.clj }}

Using `average-damp`, we can reformulate the square-root procedure as follows:

{{ 1.3.4.3.clj }}

Notice how this formulation makes explicit the three ideas in the method:
fixed-point search, average damping, and the function \\( y \mapsto x/y \\).
It is instructive to compare this formulation of the square-root method with
the original version given in section [1.1.7](book-Z-H-10.html#%_sec_1.1.7). 
Bear in mind that these procedures express the same process, and notice how much
clearer the idea becomes when we express the process in terms of these abstractions.
In general, there are many ways to formulate a process as a procedure. Experienced
programmers know how to choose procedural formulations that are particularly 
perspicuous, and where useful elements of the process are exposed as separate 
entities that can be reused in other applications. As a simple example of reuse, 
notice that the cube root of _x_ is a fixed point of the function \\(y \mapsto x/y^{2}\\),
so we can immediately generalize our square-root procedure to one that extracts cube roots:60

{{ 1.3.4.4.clj }}

#### [Newton's method](book-Z-H-4.html#sec_Temp_116)

When we first introduced the square-root procedure, in section
[1.1.7](book-Z-H-10.html#%_sec_1.1.7), we mentioned that this was a special
case of _Newton's method_. If \\( x \mapsto g(x) \\) is a
differentiable function, then a solution of the equation \\( g(x)=0 \\)
is a fixed point of the function \\( x \mapsto f(x) \\) where

\\[ f(x) = x - \frac{g(x)}{Dg(x)} \\]

and \\( Dg(x) \\) is the derivative of _g_ evaluated at _x_. Newton's method is
the use of the fixed-point method we saw above to approximate a solution of
the equation by finding a fixed point of the function _f_.61 For many
functions _g_ and for sufficiently good initial guesses for _x_, Newton's
method converges very rapidly to a solution of \\(g(x) = 0\\).62

In order to implement Newton's method as a procedure, we must first express
the idea of derivative. Note that "derivative," like average damping, is
something that transforms a function into another function. For instance, the
derivative of the function \\( x \mapsto x^{3} \\) is the function
\\( x \mapsto 3x^{2} \\). In general, if _g_ is a function and _dx_ is a
small number, then the derivative _Dg_ of _g_ is the function whose value at
any number _x_ is given (in the limit of small _dx_) by

\\[ Dg(x) = \frac{g(x+dx)-g(x)}{dx} \\]

Thus, we can express the idea of derivative (taking _dx_ to be, say,
0.00001) as the procedure

{{ 1.3.4.5.clj }}

Like `average-damp`, `deriv` is a procedure that takes a procedure as argument
and returns a procedure as value. For example, to approximate the derivative
of \\( x \mapsto x^{3} \\) at 5 (whose exact value is 75) we can
evaluate

{{ 1.3.4.6.clj }}

With the aid of `deriv`, we can express Newton's method as a fixed-point
process:

{{ 1.3.4.7.clj }}

The `newton-transform` procedure expresses the formula at the beginning of
this section, and `newtons-method` is readily defined in terms of this. It
takes as arguments a procedure that computes the function for which we want to
find a zero, together with an initial guess. For instance, to find the square
root of _x_, we can use Newton's method to find a zero of the function
\\( y \mapsto y^{2}-x \\) starting with an initial guess of 1.63 This
provides yet another form of the square-root procedure:

{{ 1.3.4.8.clj }}

#### [Abstractions and first-class procedures](book-Z-H-4.html#sec_Temp_120)

We've seen two ways to express the square-root computation as an instance of a
more general method, once as a fixed-point search and once using Newton's
method. Since Newton's method was itself expressed as a fixed-point process,
we actually saw two ways to compute square roots as fixed points. Each method
begins with a function and finds a fixed point of some transformation of the
function. We can express this general idea itself as a procedure:

{{ 1.3.4.9.clj }}

This very general procedure takes as its arguments a procedure `g` that
computes some function, a procedure that transforms `g`, and an initial guess.
The returned result is a fixed point of the transformed function.

Using this abstraction, we can recast the first square-root computation from
this section (where we look for a fixed point of the average-damped version of
\\( y \mapsto x/y \\)) as an instance of this general method:

{{ 1.3.4.10.clj }}

Similarly, we can express the second square-root computation from this section
(an instance of Newton's method that finds a fixed point of the Newton
transform of \\( y \mapsto y^{2}-x \\)) as

{{ 1.3.4.11.clj }}

We began section 1.3 with the observation that compound procedures are a
crucial abstraction mechanism, because they permit us to express general
methods of computing as explicit elements in our programming language. Now
we've seen how higher-order procedures permit us to manipulate these general
methods to create further abstractions.

As programmers, we should be alert to opportunities to identify the underlying
abstractions in our programs and to build upon them and generalize them to
create more powerful abstractions. This is not to say that one should always
write programs in the most abstract way possible; expert programmers know how
to choose the level of abstraction appropriate to their task. But it is
important to be able to think in terms of these abstractions, so that we can
be ready to apply them in new contexts. The significance of higher-order
procedures is that they enable us to represent these abstractions explicitly
as elements in our programming language, so that they can be handled just like
other computational elements.

In general, programming languages impose restrictions on the ways in which
computational elements can be manipulated. Elements with the fewest
restrictions are said to have _first-class_ status. Some of the "rights and
privileges" of first-class elements are:64

  * They may be named by variables. 
  * They may be passed as arguments to procedures. 
  * They may be returned as the results of procedures. 
  * They may be included in data structures.65

Lisp, unlike other common programming languages, awards procedures full first-
class status. This poses challenges for efficient implementation, but the
resulting gain in expressive power is enormous.66

**Exercise 1.40.**  Define a procedure `cubic` that can be used together with the
`newtons-method` procedure in expressions of the form

{{ 1.3.4.12.clj }}

to approximate zeros of the cubic \\( x^{3} + ax^{2} + bx + c \\).

**Exercise 1.41.**  Define a procedure `double-it` that takes a procedure of one argument
as argument and returns a procedure that applies the original procedure twice. For example,
if `inc` is a procedure that adds 1 to its argument, then `(double-it inc)` should be a
procedure that adds 2. What value is returned by

{{ 1.3.4.13.clj }}

**Exercise 1.42.**  Let _f_ and _g_ be two one-argument functions. The _composition_ _f_ after
_g_ is defined to be the function \\( x \mapsto f(g(x)) \\). Define a procedure `compose` that
implements composition. For example, if `inc` is a procedure that adds 1 to its argument, 

{{ 1.3.4.14.clj }}

**Exercise 1.43.**  If _f_ is a numerical function and _n_ is a positive integer, then we can 
form the _n_th repeated application of _f_, which is defined to be the function whose value at
 _x_ is \\( f( f(\dotsc(f(x))\dotsc))  \\). For example, if _f_ is the function 
\\( x \mapsto x+1 \\), then the _n_th repeated application of _f_ is the function
\\( x \mapsto x+n \\). If _f_ is the operation of squaring a number, then the _n_th repeated
application of _f_ is the function that raises its argument to the \\( 2^{n} \\)th power. Write
a procedure that takes as inputs a procedure that computes _f_ and a positive integer _n_ and
returns the procedure that computes the _n_th repeated application of _f_. Your procedure should
be able to be used as follows:

{{ 1.3.4.15.clj }}

Hint: You may find it convenient to use `compose` from exercise 1.42.

**Exercise 1.44.**  The idea of _smoothing_ a function is an important concept in signal processing.
If _f_ is a function and _dx_ is some small number, then the smoothed version of _f_ is the function
whose value at a point _x_ is the average of \\( f(x-dx) \\), \\( f(x) \\), and \\(f(x+dx)\\).
Write a procedure `smooth` that takes as input a procedure that computes _f_ and returns a procedure
that computes the smoothed _f_. It is sometimes valuable to repeatedly smooth a function (that is,
smooth the smoothed function, and so on) to obtained the _n-fold smoothed function_. Show how to
generate the _n_-fold smoothed function of any given function using `smooth` and `repeated` from
exercise 1.43. 

**Exercise 1.45.**  We saw in section 1.3.3 that attempting to compute square roots by naively
finding a fixed point of \\( y \mapsto x/y \\) does not converge, and that this can be fixed by
average damping. The same method works for finding cube roots as fixed points of the average-damped
\\( y \mapsto x/y^{2} \\). Unfortunately, the process does not work for fourth roots -- a single 
average damp is not enough to make a fixed-point search for \\( y \mapsto x/y^{3} \\) converge.
On the other hand, if we average damp twice (i.e., use the average damp of the average damp of
\\( y \mapsto x/y^{3}\\)) the fixed-point search does converge. Do some experiments to determine
how many average damps are required to compute _n_th roots as a fixed-point search based upon
repeated average damping of \\( y \mapsto x/y^{n-1} \\). Use this to implement a simple procedure
for computing _n_th roots using `fixed-point`, `average-damp`, and the `repeated` procedure of 
exercise 1.43. Assume that any arithmetic operations you need are available as primitives. 

**Exercise 1.46.**  Several of the numerical methods described in this chapter are instances of an
extremely general computational strategy known as _iterative improvement_. Iterative improvement
says that, to compute something, we start with an initial guess for the answer, test if the guess
is good enough, and otherwise improve the guess and continue the process using the improved guess
as the new guess. Write a procedure `iterative-improve` that takes two procedures as arguments: a
method for telling whether a guess is good enough and a method for improving a guess. 
`Iterative-improve` should return as its value a procedure that takes a guess as argument and keeps
improving the guess until it is good enough. Rewrite the `sqrt` procedure of section 
[1.1.7](book-Z-H-10.html#%_sec_1.1.7) and the `fixed-point` procedure of section 1.3.3 in terms of
`iterative-improve`. 

* * *

49 This series, usually written in the equivalent form
(![](book-Z-G-D-9.gif)/4) = 1 - (1/3) + (1/5) - (1/7) + `***`, is due to
Leibniz. We'll see how to use this as the basis for some fancy numerical
tricks in section [3.5.3](book-Z-H-24.html#%_sec_3.5.3).

50 Notice that we have used block structure (section
[1.1.8](book-Z-H-10.html#%_sec_1.1.8)) to embed the definitions of `pi-next`
and `pi-term` within `pi-sum`, since these procedures are unlikely to be
useful for any other purpose. We will see how to get rid of them altogether in
section 1.3.2.

51 The intent of exercises 1.31-1.33 is to demonstrate the expressive power
that is attained by using an appropriate abstraction to consolidate many
seemingly disparate operations. However, though accumulation and filtering are
elegant ideas, our hands are somewhat tied in using them at this point since
we do not yet have data structures to provide suitable means of combination
for these abstractions. We will return to these ideas in section
[2.2.3](book-Z-H-15.html#%_sec_2.2.3) when we show how to use _sequences_ as
interfaces for combining filters and accumulators to build even more powerful
abstractions. We will see there how these methods really come into their own
as a powerful and elegant approach to designing programs.

52 This formula was discovered by the seventeenth-century English
mathematician John Wallis.

53 It would be clearer and less intimidating to people learning Lisp if a name
more obvious than `lambda`, such as `make-procedure`, were used. But the
convention is firmly entrenched. The notation is adopted from the
![](book-Z-G-D-6.gif) calculus, a mathematical formalism introduced by the
mathematical logician Alonzo Church (1941). Church developed the
![](book-Z-G-D-6.gif) calculus to provide a rigorous foundation for studying
the notions of function and function application. The ![](book-Z-G-D-6.gif)
calculus has become a basic tool for mathematical investigations of the
semantics of programming languages.

54 Understanding internal definitions well enough to be sure a program means
what we intend it to mean requires a more elaborate model of the evaluation
process than we have presented in this chapter. The subtleties do not arise
with internal definitions of procedures, however. We will return to this issue
in section [4.1.6](book-Z-H-26.html#%_sec_4.1.6), after we learn more about
evaluation.

55 We have used 0.001 as a representative "small" number to indicate a
tolerance for the acceptable error in a calculation. The appropriate tolerance
for a real calculation depends upon the problem to be solved and the
limitations of the computer and the algorithm. This is often a very subtle
consideration, requiring help from a numerical analyst or some other kind of
magician.

56 This can be accomplished using `error`, which takes as arguments a number
of items that are printed as error messages.

57 Try this during a boring lecture: Set your calculator to radians mode and
then repeatedly press the `cos` button until you obtain the fixed point.

58 ![](book-Z-G-D-17.gif) (pronounced "maps to") is the mathematician's way
of writing `lambda`. _y_ ![](book-Z-G-D-17.gif) _x_/_y_ means `(lambda(y) (/ x
y))`, that is, the function whose value at _y_ is _x_/_y_.

59 Observe that this is a combination whose operator is itself a combination.
Exercise [1.4](book-Z-H-10.html#%_thm_1.4) already demonstrated the ability to
form such combinations, but that was only a toy example. Here we begin to see
the real need for such combinations -- when applying a procedure that is
obtained as the value returned by a higher-order procedure.

60 See exercise 1.45 for a further generalization.

61 Elementary calculus books usually describe Newton's method in terms of the
sequence of approximations _x__n_+1 = _x__n_ \- _g_(_x__n_)/_D__g_(_x__n_).
Having language for talking about processes and using the idea of fixed points
simplifies the description of the method.

62 Newton's method does not always converge to an answer, but it can be shown
that in favorable cases each iteration doubles the number-of-digits accuracy
of the approximation to the solution. In such cases, Newton's method will
converge much more rapidly than the half-interval method.

63 For finding square roots, Newton's method converges rapidly to the correct
solution from any starting point.

64 The notion of first-class status of programming-language elements is due to
the British computer scientist Christopher Strachey (1916-1975).

65 We'll see examples of this after we introduce data structures in chapter
2\.

66 The major implementation cost of first-class procedures is that allowing
procedures to be returned as values requires reserving storage for a
procedure's free variables even while the procedure is not executing. In the
Scheme implementation we will study in section
[4.1](book-Z-H-26.html#%_sec_4.1), these variables are stored in the
procedure's environment.
